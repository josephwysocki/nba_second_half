{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking Second Half Scoring Contributions in the NBA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction  \n",
    "On June 16, 2021, the Philadelphia 76ers played the Atlanta Hawks in Game 5 of the Eastern Conference Semifinals. The Sixers surrender a massive second half lead and ended up losing to the Hawks. Remarkably, in the entire second half, [only two Sixers players scored a field goal](https://www.bardown.com/somehow-only-two-2-sixers-players-managed-to-score-in-the-entire-2nd-half-of-game-5-1.1655958). Just how rare is that? Let's get an answer using some code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "from IPython.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Unique Game IDs  \n",
    "To scrape the data that we want, we need to get the proper html links for each game. Each game id is unique, and we can find them stored in the html code of specific webpgaes. These webpages are sorted by year and then month, so we need to loop through each combination of year and month, and then go to that specific webpage. There, we will find the game ids we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28019"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will start with a blank list of Game IDs\n",
    "# This is where we will store the id info as it is retrieved\n",
    "game_id_links = []\n",
    "\n",
    "# Now we need a list of years\n",
    "# We will start with a blank list and then fill it out in a loop\n",
    "years = []\n",
    "for i in range(2000,2022):\n",
    "    years.append(str(i))\n",
    "\n",
    "# We also need a list of months\n",
    "months = [\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\"]\n",
    "\n",
    "# We now have a list of years and a list of months\n",
    "# We will go through each combination and scrape the desired info\n",
    "\n",
    "# For each year ...\n",
    "for i in years:\n",
    "    # For each month ...\n",
    "    for j in months:\n",
    "        # Create a string of the required webpage link \n",
    "        c_link = 'https://www.basketball-reference.com/leagues/NBA_{}_games-{}.html'.format(i,j)\n",
    "        # Scrape the page, make it pretty, and reduce it to the basic info we want \n",
    "        r = requests.get(c_link)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data, parse_only=SoupStrainer(\"td\", {\"data-stat\":\"box_score_text\"}))\n",
    "        x = soup.findAll(\"a\")\n",
    "        # For each tag, strip out the href string (which is the game id) and add it to our list\n",
    "        for tr in x:\n",
    "            links = tr.get('href')\n",
    "            game_id_links.append(links)\n",
    "\n",
    "# Print out the length of our list to see how many games it includes\n",
    "len(game_id_links)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of games! We are looking at a 21 year span here, so it's quite reasonable that there would be so many games. We need to keep the size of this data set in mind as any attempt to analyze every single game will take a **long** time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data for Each Game  \n",
    "Okay, now we have the game id for each game over the last 21 years. Awesome! Now, we need to build unique html addresses using those game ids. That will allow us to access the stats for each game, review the box scores, and calculate how many unique scorers each team had in the second half of a given game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team: ORL, Date: 2000-01-02, Scorers: 9\n",
      "Team: MIA, Date: 2000-01-02, Scorers: 6\n",
      "Team: CLE, Date: 2000-01-03, Scorers: 7\n",
      "Team: BOS, Date: 2000-01-03, Scorers: 7\n",
      "Team: CHI, Date: 2000-01-03, Scorers: 5\n",
      "Team: POR, Date: 2000-01-03, Scorers: 9\n",
      "Team: DET, Date: 2000-01-03, Scorers: 7\n",
      "Team: ORL, Date: 2000-01-03, Scorers: 8\n",
      "Team: MIL, Date: 2000-01-03, Scorers: 5\n",
      "Team: PHI, Date: 2000-01-03, Scorers: 8\n",
      "Team: DEN, Date: 2000-01-03, Scorers: 4\n",
      "Team: UTA, Date: 2000-01-03, Scorers: 9\n",
      "Team: WAS, Date: 2000-01-03, Scorers: 8\n",
      "Team: GSW, Date: 2000-01-03, Scorers: 7\n",
      "Team: SAC, Date: 2000-01-04, Scorers: 6\n",
      "Team: CLE, Date: 2000-01-04, Scorers: 8\n",
      "Team: DAL, Date: 2000-01-04, Scorers: 7\n",
      "Team: DEN, Date: 2000-01-04, Scorers: 6\n",
      "Team: SEA, Date: 2000-01-04, Scorers: 7\n",
      "Team: HOU, Date: 2000-01-04, Scorers: 6\n"
     ]
    }
   ],
   "source": [
    "# We will start with a blank dictionary\n",
    "# This is where we will store the information as we scrape it\n",
    "# A dictionary is a good tool here as it will allow us to build a clean data frame at the end\n",
    "game_level_data = {}\n",
    "\n",
    "# As noted above, we have a lot of games in our data set. \n",
    "# That means it will take a very long time for or code to run.\n",
    "# For now, we'll just look at the first 10 games as a proof of concept.\n",
    "# Later, we can run this on the entire data set.\n",
    "\n",
    "# For each of the first 10 games...\n",
    "for i in game_id_links[0:10]:\n",
    "    \n",
    "    # Extract the date from the game id\n",
    "    # We'll use this as part of storing our results\n",
    "    date = i.split('/')[2][0:8]\n",
    "    \n",
    "    # We need team abberivations ('ATL' for Atlanta, etc) in order to properly parse the html code\n",
    "    # To grab these, we'll first make a blank list of the home and away team abbreviations\n",
    "    both_teams = []\n",
    "    \n",
    "    # Next, we will access the webpage using the game id\n",
    "    c_link = 'https://www.basketball-reference.com{}'.format(i)\n",
    "    # Then we'll parse the html code and extract instances of the team abbreviations \n",
    "    r = requests.get(c_link)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, parse_only=SoupStrainer(\"div\", {\"id\":\"inner_nav\"}))\n",
    "    x = soup.findAll(\"a\")\n",
    "    # We'll find every instance of the team abbreviation and add it to our list\n",
    "    for tr in x:\n",
    "        links = tr.get('href')\n",
    "        both_teams.append(links)    \n",
    "    team_abs = []\n",
    "\n",
    "    # The html code is a little messy, so we need to do some cleaning to get the two team abbreviations \n",
    "    for k in range(0,len(both_teams)):\n",
    "        if both_teams[k].split(\"/\")[1] =='teams':\n",
    "            ab = both_teams[k].split(\"/\")[2]\n",
    "            team_abs.append(ab)\n",
    "        else:\n",
    "            next\n",
    "    \n",
    "    # Now that we have just the team abbrevations, we can reduce to only the unique ones and add them to our list\n",
    "    team_abs = list(set(team_abs))\n",
    "\n",
    "    # Now, for each team, we need to find the desired data\n",
    "    # First, we need to get the unique tag that we will search for in the html code\n",
    "    team_2half_string = \"box-{}-h2-basic\".format(team_abs[0])\n",
    "\n",
    "    # Using this code, we will do out BeautifulSoup magic again and get the second half box score for the given team\n",
    "    source = urllib.request.urlopen(c_link).read()\n",
    "    soup = BeautifulSoup(source,'html')\n",
    "    table = soup.find_all(\"table\", {\"id\": team_2half_string})\n",
    "    # Once we find the box score, we need to make it into a table\n",
    "    df = pd.read_html(str(table))[0]\n",
    "\n",
    "    # The data frame has a multi index\n",
    "    # We can go ahead and drop that\n",
    "    df.columns = ['_'.join(col) for col in df.columns.values]\n",
    "    \n",
    "    # That leaves some ugly column titles which we will clean up\n",
    "    new_cols= []\n",
    "    for col in df.columns.values:\n",
    "        val = col.split(\"_\")[1]\n",
    "        new_cols.append(val)\n",
    "    new_cols[0] = 'Player'\n",
    "    df.columns = new_cols\n",
    "\n",
    "    # Now our data frame has nice column names but it has some unnecessary rows\n",
    "    # We can go ahead and drop those two rows\n",
    "    df = df.query('Player != \"Reserves\" & Player != \"Team Totals\"')\n",
    "\n",
    "    # There are some odd items that may come up depending on the game\n",
    "    # We need to replace these with Nan\n",
    "    df = df.replace(\"Did Not Play\", np.nan)\n",
    "    df = df.replace(\"Not With Team\", np.nan)\n",
    "    df = df.replace(\"Did Not Dress\", np.nan)\n",
    "    df = df.replace(\"Player Suspended\", np.nan)\n",
    "\n",
    "    # Keep only the rows for players who actually played\n",
    "    df = df[df['MP'].notna()]\n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Convert FG to integer\n",
    "    df['FG'] = pd.to_numeric(df['FG'])\n",
    "\n",
    "    # We've accessed the second half box score for the given team\n",
    "    # We can now compress it down into just the info we want and store those values in a dictionary\n",
    "    \n",
    "    # Create a unique dictionary key\n",
    "    game_key = date + team_abs[0]\n",
    "    # Team\n",
    "    team = team_abs[0]\n",
    "    # Date\n",
    "    game_date = date[0:4]+\"-\"+date[4:6]+\"-\"+date[6:9]\n",
    "    # Calculate the number of unique scorers for the team\n",
    "    raw_scorers = len(df.query('FG > 0'))\n",
    "    # Caluclate the number of unique scorers as a percent of total players\n",
    "    percent_scorers = raw_scorers / len(df)\n",
    "\n",
    "    # Add the information to the dictionary\n",
    "    # Note that this is a nested dictionaty. That is key for unpacking this information later\n",
    "    game_level_data[game_key] = {'team':team_abs[0],\n",
    "                                 'date':game_date,\n",
    "                                 'raw_scorers':raw_scorers,\n",
    "                                 'percent_scorers':percent_scorers}\n",
    "    print(\"Team: {}, Date: {}, Scorers: {}\".format(team_abs[0],game_date,raw_scorers))\n",
    "    \n",
    "    # Do it all again for the second team\n",
    "    # Why wasn't this done in a loop? Because it saves us from having to visit the webpage twice\n",
    "    team_2half_string = \"box-{}-h2-basic\".format(team_abs[1])\n",
    "\n",
    "    source = urllib.request.urlopen(c_link).read()\n",
    "    soup = BeautifulSoup(source,'html')\n",
    "    table = soup.find_all(\"table\", {\"id\": team_2half_string})\n",
    "    df = pd.read_html(str(table))[0]\n",
    "\n",
    "    # Drop top level index\n",
    "    df.columns = ['_'.join(col) for col in df.columns.values]\n",
    "\n",
    "    new_cols= []\n",
    "\n",
    "    for col in df.columns.values:\n",
    "        val = col.split(\"_\")[1]\n",
    "        new_cols.append(val)\n",
    "\n",
    "    new_cols[0] = 'Player'\n",
    "\n",
    "    df.columns = new_cols\n",
    "\n",
    "    # Remove \"Reserves\" and \"Team Totals\" rows\n",
    "    df = df.query('Player != \"Reserves\" & Player != \"Team Totals\"')\n",
    "\n",
    "    # Replace \"Did Not Play\" and \"Not With Team\" with Nan\n",
    "    df = df.replace(\"Did Not Play\", np.nan)\n",
    "    df = df.replace(\"Not With Team\", np.nan)\n",
    "    df = df.replace(\"Did Not Dress\", np.nan)\n",
    "    df = df.replace(\"Player Suspended\", np.nan)\n",
    "\n",
    "    # Keep only the rows for players who actually played\n",
    "    df = df[df['MP'].notna()]\n",
    "\n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Convert FG to integer\n",
    "    df['FG'] = pd.to_numeric(df['FG'])\n",
    "\n",
    "    # Dictionary Key\n",
    "    game_key = date + team_abs[1]\n",
    "    # Team\n",
    "    team = team_abs[1]\n",
    "    # Date\n",
    "    game_date = date[0:4]+\"-\"+date[4:6]+\"-\"+date[6:9]\n",
    "    # Raw Scorers\n",
    "    raw_scorers = len(df.query('FG > 0'))\n",
    "    # Percent Scorers\n",
    "    percent_scorers = raw_scorers / len(df)\n",
    "\n",
    "    # Save Info\n",
    "    game_level_data[game_key] = {'team':team_abs[1],\n",
    "                                 'date':game_date,\n",
    "                                 'raw_scorers':raw_scorers,\n",
    "                                 'percent_scorers':percent_scorers}\n",
    "    \n",
    "    # Print a general statement to know that the code is running\n",
    "    print(\"Team: {}, Date: {}, Scorers: {}\".format(team_abs[1],game_date,raw_scorers))\n",
    "    \n",
    "    # We will sleep for a tenth of a second to reduce to speed of requests we make to the webpage\n",
    "    time.sleep(0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Up  \n",
    "Great! Our code worked and we were able to run through the first 10 games of our data set. Now, we need to unpack our dictionary into a data frame so that we can analyze everything and get an answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>date</th>\n",
       "      <th>raw_scorers</th>\n",
       "      <th>percent_scorers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20000103DEN</th>\n",
       "      <td>DEN</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>4</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103CHI</th>\n",
       "      <td>CHI</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103MIL</th>\n",
       "      <td>MIL</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000104HOU</th>\n",
       "      <td>HOU</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000102MIA</th>\n",
       "      <td>MIA</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000104DEN</th>\n",
       "      <td>DEN</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000104SAC</th>\n",
       "      <td>SAC</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103CLE</th>\n",
       "      <td>CLE</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>7</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103BOS</th>\n",
       "      <td>BOS</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>7</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103DET</th>\n",
       "      <td>DET</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000104DAL</th>\n",
       "      <td>DAL</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000104SEA</th>\n",
       "      <td>SEA</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103GSW</th>\n",
       "      <td>GSW</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000104CLE</th>\n",
       "      <td>CLE</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103PHI</th>\n",
       "      <td>PHI</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>8</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103ORL</th>\n",
       "      <td>ORL</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103WAS</th>\n",
       "      <td>WAS</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103UTA</th>\n",
       "      <td>UTA</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000103POR</th>\n",
       "      <td>POR</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000102ORL</th>\n",
       "      <td>ORL</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            team        date  raw_scorers  percent_scorers\n",
       "20000103DEN  DEN  2000-01-03            4         0.400000\n",
       "20000103CHI  CHI  2000-01-03            5         0.454545\n",
       "20000103MIL  MIL  2000-01-03            5         0.625000\n",
       "20000104HOU  HOU  2000-01-04            6         0.666667\n",
       "20000102MIA  MIA  2000-01-02            6         0.857143\n",
       "20000104DEN  DEN  2000-01-04            6         0.666667\n",
       "20000104SAC  SAC  2000-01-04            6         0.666667\n",
       "20000103CLE  CLE  2000-01-03            7         0.700000\n",
       "20000103BOS  BOS  2000-01-03            7         0.777778\n",
       "20000103DET  DET  2000-01-03            7         0.875000\n",
       "20000104DAL  DAL  2000-01-04            7         0.875000\n",
       "20000104SEA  SEA  2000-01-04            7         0.875000\n",
       "20000103GSW  GSW  2000-01-03            7         0.875000\n",
       "20000104CLE  CLE  2000-01-04            8         0.888889\n",
       "20000103PHI  PHI  2000-01-03            8         0.800000\n",
       "20000103ORL  ORL  2000-01-03            8         0.666667\n",
       "20000103WAS  WAS  2000-01-03            8         0.666667\n",
       "20000103UTA  UTA  2000-01-03            9         0.750000\n",
       "20000103POR  POR  2000-01-03            9         0.750000\n",
       "20000102ORL  ORL  2000-01-02            9         0.818182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can quickly convert our dicitonary to a data frame\n",
    "# We will sort by raw scorers as that is what  we are most interested in seeing\n",
    "pd.DataFrame.from_dict(game_level_data, orient='index').sort_values('raw_scorers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have exactly what we were looking for. We now have a list of games with two specific measurements (raw scorers and percent scorers) for each game. We can run this code across our entire set of games and see just how rare that Sixers performance really was."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
